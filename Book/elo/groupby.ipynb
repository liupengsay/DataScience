{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 借助groupby进行特征提取\n",
    "## 从原始数据输入到特征文件生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.引入三方库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.读取所需数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test =  pd.read_csv('data/test.csv')\n",
    "merchant = pd.read_csv('data/merchants.csv')\n",
    "new_transaction = pd.read_csv('data/new_merchant_transactions.csv')\n",
    "history_transaction = pd.read_csv('data/historical_transactions.csv')\n",
    "\n",
    "# 按照字典序进行编码\n",
    "def change_object_cols(se):\n",
    "    value = se.unique().tolist()\n",
    "    value.sort()\n",
    "    return se.map(pd.Series(range(len(value)), index=value)).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.做数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train&test\n",
    "# 对首次活跃月份进行编码\n",
    "se_map = change_object_cols(train['first_active_month'].append(test['first_active_month']).astype(str))\n",
    "train['first_active_month'] = se_map[:train.shape[0]]\n",
    "test['first_active_month'] = se_map[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 商户信息\n",
    "# 1、根据业务含义划分离散字段category_cols与连续字段numeric_cols。\n",
    "category_cols = ['merchant_id', 'merchant_group_id', 'merchant_category_id',\n",
    "       'subsector_id', 'category_1',\n",
    "       'most_recent_sales_range', 'most_recent_purchases_range',\n",
    "       'category_4', 'city_id', 'state_id', 'category_2']\n",
    "numeric_cols = ['numerical_1', 'numerical_2',\n",
    "     'avg_sales_lag3', 'avg_purchases_lag3', 'active_months_lag3',\n",
    "       'avg_sales_lag6', 'avg_purchases_lag6', 'active_months_lag6',\n",
    "       'avg_sales_lag12', 'avg_purchases_lag12', 'active_months_lag12']\n",
    "\n",
    "# 2、对非数值型的离散字段进行字典排序编码。\n",
    "for col in ['category_1', 'most_recent_sales_range', 'most_recent_purchases_range', 'category_4']:\n",
    "    merchant[col] = change_object_cols(merchant[col])\n",
    "    \n",
    "# 3、为了能够更方便统计，进行缺失值的处理，对离散字段统一用-1进行填充。\n",
    "merchant[category_cols] = merchant[category_cols].fillna(-1)\n",
    "\n",
    "\n",
    "# 4、对离散型字段探查发现有正无穷值，这是特征提取以及模型所不能接受的，因此需要对无限值进行处理，此处采用最大值进行替换。\n",
    "inf_cols = ['avg_purchases_lag3', 'avg_purchases_lag6', 'avg_purchases_lag12']\n",
    "merchant[inf_cols] = merchant[inf_cols].replace(np.inf, merchant[inf_cols].replace(np.inf, -99).max().max())\n",
    "\n",
    "# 5、对于离散字段的缺失值处理方式也有多样，这里先使用平均值进行填充，后续有需要再进行优化处理。\n",
    "for col in numeric_cols:\n",
    "    merchant[col] = merchant[col].fillna(merchant[col].mean())\n",
    "    \n",
    "# 6、去除与transaction交易记录表格重复的列，以及merchant_id的重复记录。\n",
    "duplicate_cols = ['merchant_id', 'merchant_category_id', 'subsector_id', 'category_1', 'city_id', 'state_id', 'category_2']\n",
    "merchant = merchant.drop(duplicate_cols[1:], axis=1)\n",
    "merchant = merchant.loc[merchant['merchant_id'].drop_duplicates().index.tolist()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 交易记录\n",
    "# 1、为了统一处理，首先拼接new和history两张表格，后续可以month_lag>=0进行区分。\n",
    "transaction = pd.concat([new_transaction, history_transaction], axis=0, ignore_index=True)\n",
    "del new_transaction\n",
    "del history_transaction\n",
    "gc.collect()\n",
    "\n",
    "# 2、同样划分离散字段、连续字段以及时间字段。\n",
    "numeric_cols = [ 'installments', 'month_lag', 'purchase_amount']\n",
    "category_cols = ['authorized_flag', 'card_id', 'city_id', 'category_1',\n",
    "       'category_3', 'merchant_category_id', 'merchant_id', 'category_2', 'state_id',\n",
    "       'subsector_id']\n",
    "time_cols = ['purchase_date']\n",
    "\n",
    "# 3、可仿照merchant的处理方式对字符型的离散特征进行字典序编码以及缺失值填充。\n",
    "for col in ['authorized_flag', 'category_1', 'category_3']:\n",
    "    transaction[col] = change_object_cols(transaction[col].fillna(-1).astype(str))\n",
    "transaction[category_cols] = transaction[category_cols].fillna(-1)\n",
    "transaction['category_2'] = transaction['category_2'].astype(int)\n",
    "\n",
    "# 4、进行时间段的处理，简单起见进行月份、日期的星期数（工作日与周末）、以及\n",
    "# 时间段（上午、下午、晚上、凌晨）的信息提取。\n",
    "transaction['purchase_month'] = transaction['purchase_date'].apply(lambda x:'-'.join(x.split(' ')[0].split('-')[:2]))\n",
    "transaction['purchase_hour_section'] = transaction['purchase_date'].apply(lambda x: x.split(' ')[1].split(':')[0]).astype(int)//6\n",
    "transaction['purchase_day'] = transaction['purchase_date'].apply(lambda x: datetime.strptime(x.split(\" \")[0], \"%Y-%m-%d\").weekday())//5                                                                    \n",
    "del transaction['purchase_date']\n",
    "\n",
    "\n",
    "# 5、对新生成的购买月份离散字段进行字典序编码。\n",
    "transaction['purchase_month'] = change_object_cols(transaction['purchase_month'].fillna(-1).astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 表格合并\n",
    "# 6、处理完transaction与merchant两个表格后，为了方便特征的统一计算将其merge合并，重新划分相应字段种类。\n",
    "cols = ['merchant_id', 'most_recent_sales_range', 'most_recent_purchases_range', 'category_4']\n",
    "transaction = pd.merge(transaction, merchant[cols], how='left', on='merchant_id')\n",
    "\n",
    "numeric_cols = ['purchase_amount', 'installments']\n",
    "\n",
    "category_cols = ['authorized_flag', 'city_id', 'category_1',\n",
    "       'category_3', 'merchant_category_id','month_lag','most_recent_sales_range',\n",
    "                 'most_recent_purchases_range', 'category_4',\n",
    "                 'purchase_month', 'purchase_hour_section', 'purchase_day']\n",
    "\n",
    "id_cols = ['card_id', 'merchant_id']\n",
    "\n",
    "\n",
    "transaction['purchase_day_diff'] = transaction.groupby(\"card_id\")['purchase_day'].diff()\n",
    "transaction['purchase_month_diff'] = transaction.groupby(\"card_id\")['purchase_month'].diff()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.进行特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 根据字段的种类设置相应想获取的统计量\n",
    "numeric_cols = ['authorized_flag',  'category_1', 'installments',\n",
    "       'category_3',  'month_lag','purchase_month','purchase_day','purchase_day_diff', 'purchase_month_diff',\n",
    "       'purchase_amount', 'category_2', \n",
    "       'purchase_month', 'purchase_hour_section', 'purchase_day',\n",
    "       'most_recent_sales_range', 'most_recent_purchases_range', 'category_4']\n",
    "categorical_cols = ['city_id', 'merchant_category_id', 'merchant_id', 'state_id', 'subsector_id']\n",
    "aggs = {}\n",
    "for col in numeric_cols:\n",
    "    aggs[col] = ['nunique', 'mean', 'min', 'max','var','skew', 'sum']\n",
    "for col in categorical_cols:\n",
    "    aggs[col] = ['nunique']    \n",
    "aggs['card_id'] = ['size', 'count']\n",
    "cols = ['card_id']\n",
    "for key in aggs.keys():\n",
    "    cols.extend([key+'_'+stat for stat in aggs[key]])\n",
    "\n",
    "df = transaction[transaction['month_lag']<0].groupby('card_id').agg(aggs).reset_index()\n",
    "df.columns = cols[:1] + [co+'_hist' for co in cols[1:]]\n",
    "\n",
    "df2 = transaction[transaction['month_lag']>=0].groupby('card_id').agg(aggs).reset_index()\n",
    "df2.columns = cols[:1] + [co+'_new' for co in cols[1:]]\n",
    "df = pd.merge(df, df2, how='left',on='card_id')\n",
    "\n",
    "df2 = transaction.groupby('card_id').agg(aggs).reset_index()\n",
    "df2.columns = cols\n",
    "df = pd.merge(df, df2, how='left',on='card_id')\n",
    "del transaction\n",
    "gc.collect()\n",
    "\n",
    "# 生成训练集与测试集\n",
    "train = pd.merge(train, df, how='left', on='card_id')\n",
    "test =  pd.merge(test, df, how='left', on='card_id')\n",
    "del df\n",
    "train.to_csv(\"preprocess/train_groupby.csv\", index=False)\n",
    "test.to_csv(\"preprocess/test_groupby.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
